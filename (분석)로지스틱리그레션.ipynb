{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_extractor(corpus):  \n",
    "    # returns a frequency-based DTM\n",
    "    vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1)) \n",
    "    features = vectorizer.fit_transform(corpus) # transform texts to a frequency matrix\n",
    "    return vectorizer, features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_extractor(corpus):\n",
    "    # returns a tf-idf based DTM\n",
    "    vectorizer = TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/soothingni/Downloads/NOUN_전체.txt', 'r') as f:\n",
    "    total_docs = [x.strip().split(',') for x in f.readlines()]\n",
    "    docs =  [(int(doc[3]), doc[4]) for doc in total_docs if len(doc) == 4]\n",
    "    scores, texts = zip(*docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_texts = []\n",
    "filtered_labels = []\n",
    "\n",
    "for score, text in zip(scores, texts):\n",
    "    if 100 <= score <= 1000:\n",
    "        continue\n",
    "        \n",
    "    # 평점 기준으로 문서에 label을 부여\n",
    "    # 0~100 -> 부정, 0\n",
    "    # 1000~ -> 긍정, 1\n",
    "    filtered_texts.append(text)\n",
    "    filtered_labels.append(1 if score > 1000 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_reviews = len(filtered_texts) #788189\n",
    "\n",
    "num_train = int(num_reviews*0.7) #551732\n",
    "# 전체 리뷰 중에서 70%를 training data로 사용하고, 나머지 30%를 test data로 사용\n",
    "train_texts = filtered_texts[:num_train]\n",
    "train_labels = filtered_labels[:num_train]\n",
    "test_texts = filtered_texts[num_train+1:]\n",
    "test_labels = filtered_labels[num_train+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the following method\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(filtered_texts, filtered_labels, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer, train_tf_features = tf_extractor(train_texts)\n",
    "# input의 형태 = list of docs\n",
    "test_tf_features = tf_vectorizer.transform(test_texts)\n",
    "vocablist = [word for word, _ in sorted(tf_vectorizer.vocabulary_.items(), key=lambda x:x[1])]\n",
    "# tf_vectorizer.vocabulary_.items() returns a list of (word, frequency)\n",
    "# We sort words based on their frequencies and save the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 758 out of 30708\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# tf matrix를 사용한 경우\n",
    "lr = LogisticRegression(C=0.1, penalty='l2', solver='sag') # Lasso regression\n",
    "# C = Inverse of regularization strength(==lamda), 즉 C 값이 작을수록 penalty를 많이 준다는 것입니다.\n",
    "#C는 hyper parameter로, 학습을 통해 도출할 수 없음 / 사람이 지정해야함; 5가지 정도를 실행해보고 가장 정확한 결과를 도출하는 것을 활용\n",
    "# penalty를 많이 준다는 뜻은 L1 같은 경우는 feature의 수를 그만큼 많이 줄인다는 뜻이고\n",
    "# L2인 경우는 weight 값을 더 0에 가깝게 한다는 뜻입니다.\n",
    "lr.fit(train_tf_features, train_labels) # 학습\n",
    "pred_labels = lr.predict(test_tf_features)\n",
    "print('Misclassified samples: {} out of {}'.format((pred_labels != test_labels).sum(),len(test_labels)))\n",
    "print('Accuracy: %.2f' % accuracy_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified samples: 756 out of 30708\n",
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# tfidf matrix를 사용한 경우\n",
    "tfidf_vectorizer, train_tfidf_features = tfidf_extractor(train_texts)\n",
    "test_tfidf_features = tfidf_vectorizer.transform(test_texts)\n",
    "lr = LogisticRegression(C=0.1, penalty='l1', solver='saga') # Lasso regression\n",
    "lr.fit(train_tfidf_features, train_labels) # 학습\n",
    "pred_labels = lr.predict(test_tfidf_features)\n",
    "print('Misclassified samples: {} out of {}'.format((pred_labels != test_labels).sum(),len(test_labels)))\n",
    "print('Accuracy: %.2f' % accuracy_score(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(52582, 3.900098221144166), (10963, 3.0996777281245906), (489, 2.934401698800163), (39051, 2.793706587186403), (35440, 2.4334745441335985)]\n",
      "저희 (3.900)\n",
      "내용 (3.100)\n",
      "가해자 (2.934)\n",
      "아이 (2.794)\n",
      "수사 (2.433)\n",
      "아들 (2.378)\n",
      "사건 (2.123)\n",
      "검찰 (1.988)\n",
      "주민 (1.982)\n",
      "대한 (1.920)\n",
      "일본 (1.578)\n",
      "청원 (1.565)\n",
      "병원 (1.295)\n",
      "일부 (0.619)\n",
      "상황 (0.502)\n",
      "가족 (0.274)\n",
      "피해자 (0.258)\n",
      "치료 (0.130)\n",
      "처벌 (0.098)\n",
      "가가 (0.000)\n",
      "가가호호 (0.000)\n",
      "가각 (0.000)\n",
      "가감 (0.000)\n",
      "가거대교 (0.000)\n",
      "가거도 (0.000)\n",
      "가건물 (0.000)\n",
      "가게 (0.000)\n",
      "가격 (0.000)\n",
      "가격결정 (0.000)\n",
      "가격담합 (0.000)\n",
      "가격탄력성 (0.000)\n",
      "가격표 (0.000)\n",
      "가견 (0.000)\n",
      "가결 (0.000)\n",
      "가결의 (0.000)\n",
      "가경 (0.000)\n",
      "가경제 (0.000)\n",
      "가계 (0.000)\n",
      "가계도 (0.000)\n",
      "가계부 (0.000)\n",
      "가계부채 (0.000)\n",
      "가계비 (0.000)\n",
      "가계수표 (0.000)\n",
      "가계약 (0.000)\n",
      "가곘습까 (0.000)\n",
      "가고시마 (0.000)\n",
      "가고시마공항 (0.000)\n",
      "가고일 (0.000)\n",
      "가곡 (0.000)\n",
      "가공 (0.000)\n",
      "힘들곘습 (0.000)\n",
      "힘들껍니 (0.000)\n",
      "힘들닙다 (0.000)\n",
      "힘들덴데 (0.000)\n",
      "힘듧니 (0.000)\n",
      "힘럾 (0.000)\n",
      "힘를 (0.000)\n",
      "힘마 (0.000)\n",
      "힘모 (0.000)\n",
      "힘보탭시 (0.000)\n",
      "힘빠 (0.000)\n",
      "힘살 (0.000)\n",
      "힘셨으 (0.000)\n",
      "힘쌔 (0.000)\n",
      "힘쌔다 (0.000)\n",
      "힘쌘 (0.000)\n",
      "힘썻다 (0.000)\n",
      "힘안 (0.000)\n",
      "힘었는 (0.000)\n",
      "힘을썻습니 (0.000)\n",
      "힘이쌔 (0.000)\n",
      "힘일 (0.000)\n",
      "힘임 (0.000)\n",
      "힘입 (0.000)\n",
      "힘자랑 (0.000)\n",
      "힘좀 (0.000)\n",
      "힘줄 (0.000)\n",
      "힘즐 (0.000)\n",
      "힘쪽 (0.000)\n",
      "힘키 (0.000)\n",
      "힘펠 (0.000)\n",
      "힙니 (0.000)\n",
      "힙니카 (0.000)\n",
      "힙드네 (0.000)\n",
      "힙들 (0.000)\n",
      "힙듬니 (0.000)\n",
      "힙듭니 (0.000)\n",
      "힙시 (0.000)\n",
      "힙없는 (0.000)\n",
      "힙찔이들 (0.000)\n",
      "힙합 (0.000)\n",
      "힛틀러 (0.000)\n",
      "힜습니 (0.000)\n",
      "힝듭니 (0.000)\n",
      "힝생제 (0.000)\n",
      "힝클리 (0.000)\n",
      "힢습니 (0.000)\n",
      "힢을때 (0.000)\n",
      "나라 (-0.168)\n",
      "미세먼지 (-1.369)\n"
     ]
    }
   ],
   "source": [
    "# Get coefficients of the model\n",
    "coefficients = lr.coef_.tolist()\n",
    "\n",
    "sorted_coefficients = sorted(enumerate(coefficients[0]), key=lambda x:x[1], reverse=True)\n",
    "# 학습에 사용된 각 단어마다의 coefficient (즉 weight) 값이 존재\n",
    "# coefficient값이 큰 순으로 정렬 'reverse=True'\n",
    "\n",
    "print(sorted_coefficients[:5])\n",
    "# print top 50 positive words\n",
    "for word, coef in sorted_coefficients[:50]:\n",
    "    print('{0:} ({1:.3f})'.format(vocablist[word], coef))\n",
    "# print top 50 negative words\n",
    "for word, coef in sorted_coefficients[-50:]:\n",
    "    print('{0:} ({1:.3f})'.format(vocablist[word], coef))\n",
    "    \n",
    "#기계학습 텍스트분석에서 각 단어는 하나의 변수; coefficient는 모델의 parameter에 해당된다; y = b1 + b0x에서 단어(tf or tfidf)는 x, coefficent는 b0임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
